{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "---\n",
    "\n",
    "This repository uses simulated orange juice sales data from [Azure Open Datasets](https://azure.microsoft.com/services/open-datasets/) to walk you through the process of training many models and forecasting on Azure Machine Learning. \n",
    "\n",
    "This notebook walks you through all the necessary steps to configure the data for this solution accelerator, including:\n",
    "\n",
    "1. Download the sample data\n",
    "2. Split in training/forecasting sets\n",
    "3. Connect to your workspace and upload the data to its Datastore\n",
    "\n",
    "### Prerequisites\n",
    "If you have already run the [00_Setup_AML_Workspace](00_Setup_AML_Workspace.ipynb) notebook you are all set.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.0 Download sample data\n",
    "\n",
    "The time series data used in this example was simulated based on the University of Chicago's Dominick's Finer Foods dataset, which featured two years of sales of 3 different orange juice brands for individual stores. You can learn more about the dataset [here](https://azure.microsoft.com/services/open-datasets/catalog/sample-oj-sales-simulated/). \n",
    "\n",
    "The full dataset includes simulated sales for 3,991 stores with 3 orange juice brands each, thus allowing 11,973 models to be trained to showcase the power of the many models pattern. Each series contains data from '1990-06-14' to '1992-10-01'."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You'll need the `azureml-opendatasets` package to download the data. You can install it with the following:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install azureml-opendatasets"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install update azureml-opendatasets"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll start by downloading the first 10 files but you can easily edit the code below to train all 11,973 models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset_maxfiles = 10 # Set to 11973 or 0 to get all the files"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adjust Path \n",
    "This allows access to the utils folder that is not directly in the path of this folder. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import os\n",
    "from azureml.opendatasets import OjSalesSimulated\n",
    "from utils.env_variables import Env\n",
    "\n",
    "e=Env()\n",
    "\n",
    "# Pull all of the data\n",
    "oj_sales_files = OjSalesSimulated.get_file_dataset()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pull only the first `dataset_maxfiles` files\n",
    "if dataset_maxfiles:\n",
    "    oj_sales_files = oj_sales_files.take(dataset_maxfiles)\n",
    "\n",
    "# Create a folder to download\n",
    "target_path = 'oj_sales_data' \n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "# Download the data\n",
    "oj_sales_files.download(target_path, overwrite=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.0 Split data in two sets\n",
    "\n",
    "We will now split each dataset in two parts: one will be used for training, and the other will be used for simulating batch forecasting. The training files will contain the data records before '1992-5-28' and the last part of each series will be stored in the inferencing files.\n",
    "\n",
    "Finally, we will upload both sets of data files to the Workspace's default [Datastore](https://docs.microsoft.compython/api/azureml-core/azureml.core.datastore(class))."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scripts.helper import split_data\n",
    "\n",
    "# Provide name of timestamp column in the data and date from which to split into the inference dataset\n",
    "timestamp_column = e.timestample_column\n",
    "split_date = e.split_date\n",
    "\n",
    "# Split each file and store in corresponding directory\n",
    "train_path, inference_path = split_data(target_path, timestamp_column, split_date)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.0 Upload data to Datastore in AML Workspace\n",
    "\n",
    "In the [setup notebook](00_Setup_AML_Workspace.ipynb) you created a [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace). We are going to register the data in that enviroment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "from utils.aml_workspace import Connect\n",
    "\n",
    "if len(e.subscription_id) == 0:\n",
    "    subscription_id = os.environ.get(\"SUBSCRIPTION_ID\")\n",
    "else:\n",
    "    subscription_id = e.subscription_id\n",
    "\n",
    "connect = Connect(subscription_id=subscription_id)\n",
    "\n",
    "ws = connect.authenticate()\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "get_workspace error using subscription_id=, resource_group_name=coding-forge-rg, workspace_name=coding-forge-ml-ws\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ProjectSystemException",
     "evalue": "ProjectSystemException:\n\tMessage: {\n    \"error_details\": {\n        \"error\": {\n            \"code\": \"InvalidSubscriptionId\",\n            \"message\": \"The provided subscription identifier 'resourceGroups' is malformed or invalid.\"\n        }\n    },\n    \"status_code\": 400,\n    \"url\": \"https://management.azure.com/subscriptions/resourceGroups/coding-forge-rg/providers/Microsoft.MachineLearningServices/workspaces/coding-forge-ml-ws?api-version=2020-01-01\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"{\\n    \\\"error_details\\\": {\\n        \\\"error\\\": {\\n            \\\"code\\\": \\\"InvalidSubscriptionId\\\",\\n            \\\"message\\\": \\\"The provided subscription identifier 'resourceGroups' is malformed or invalid.\\\"\\n        }\\n    },\\n    \\\"status_code\\\": 400,\\n    \\\"url\\\": \\\"https://management.azure.com/subscriptions/resourceGroups/coding-forge-rg/providers/Microsoft.MachineLearningServices/workspaces/coding-forge-ml-ws?api-version=2020-01-01\\\"\\n}\"\n    }\n}",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mErrorResponseWrapperException\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/mm/lib/python3.7/site-packages/azureml/_project/_commands.py\u001b[0m in \u001b[0;36mget_workspace\u001b[0;34m(auth, subscription_id, resource_group_name, workspace_name, location, cloud, workspace_id)\u001b[0m\n\u001b[1;32m    436\u001b[0m            \u001b[0mresource_group_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m            workspace_name)\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mClientRequestError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/mm/lib/python3.7/site-packages/azureml/_base_sdk_common/workspace/operations/workspaces_operations.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, resource_group_name, workspace_name, custom_headers, raw, is_dataplane, **operation_config)\u001b[0m\n\u001b[1;32m    110\u001b[0m             raise models.ErrorResponseWrapperException(\n\u001b[0;32m--> 111\u001b[0;31m                 self._deserialize, response)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mErrorResponseWrapperException\u001b[0m: Operation returned an invalid status code 'Bad Request'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProjectSystemException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6818/3588491541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconnect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubscription_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubscription_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Take a look at Workspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/manymodels_mlops/ml_service/utils/aml_workspace.py\u001b[0m in \u001b[0;36mauthenticate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msubscription_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubscription_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mresource_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m        )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/mm/lib/python3.7/site-packages/azureml/core/workspace.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(name, auth, subscription_id, resource_group, location, cloud, id)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0m_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0m_cloud\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             _workspace_id=id)\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/mm/lib/python3.7/site-packages/azureml/core/workspace.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, subscription_id, resource_group, workspace_name, auth, _location, _disable_service_check, _workspace_id, sku, tags, _cloud)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_disable_service_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             auto_rest_workspace = _commands.get_workspace(\n\u001b[0;32m--> 210\u001b[0;31m                 auth, subscription_id, resource_group, workspace_name, _location, _cloud, _workspace_id)\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workspace_autorest_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_rest_workspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/mm/lib/python3.7/site-packages/azureml/_project/_commands.py\u001b[0m in \u001b[0;36mget_workspace\u001b[0;34m(auth, subscription_id, resource_group_name, workspace_name, location, cloud, workspace_id)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0msubscription_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_group_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             ))\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mresource_error_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWORKSPACE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/mm/lib/python3.7/site-packages/azureml/_base_sdk_common/common.py\u001b[0m in \u001b[0;36mresource_error_handling\u001b[0;34m(response_exception, resource_type)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mresponse_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_http_exception_response_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProjectSystemException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProjectSystemException\u001b[0m: ProjectSystemException:\n\tMessage: {\n    \"error_details\": {\n        \"error\": {\n            \"code\": \"InvalidSubscriptionId\",\n            \"message\": \"The provided subscription identifier 'resourceGroups' is malformed or invalid.\"\n        }\n    },\n    \"status_code\": 400,\n    \"url\": \"https://management.azure.com/subscriptions/resourceGroups/coding-forge-rg/providers/Microsoft.MachineLearningServices/workspaces/coding-forge-ml-ws?api-version=2020-01-01\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"{\\n    \\\"error_details\\\": {\\n        \\\"error\\\": {\\n            \\\"code\\\": \\\"InvalidSubscriptionId\\\",\\n            \\\"message\\\": \\\"The provided subscription identifier 'resourceGroups' is malformed or invalid.\\\"\\n        }\\n    },\\n    \\\"status_code\\\": 400,\\n    \\\"url\\\": \\\"https://management.azure.com/subscriptions/resourceGroups/coding-forge-rg/providers/Microsoft.MachineLearningServices/workspaces/coding-forge-ml-ws?api-version=2020-01-01\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will upload both sets of data files to your Workspace's default [Datastore](https://docs.microsoft.com/azure/machine-learning/how-to-access-data). \n",
    "A Datastore is a place where data can be stored that is then made accessible for training or forecasting. Please refer to [Datastore documentation](https://docs.microsoft.com/python/api/azureml-core/azureml.core.datastore(class)) on how to access data from Datastore."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Connect to default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Upload train data\n",
    "ds_train_path = target_path + '_train'\n",
    "datastore.upload(src_dir=train_path, target_path=ds_train_path, overwrite=True)\n",
    "\n",
    "# Upload inference data\n",
    "ds_inference_path = target_path + '_inference'\n",
    "datastore.upload(src_dir=inference_path, target_path=ds_inference_path, overwrite=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### *[Optional]* If data is already in Azure: create Datastore from it\n",
    "\n",
    "If your data is already in Azure you don't need to upload it from your local machine to the default datastore. Instead, you can create a new Datastore that references that set of data. \n",
    "The following is an example of how to set up a Datastore from a container in Blob storage where the sample data is located. \n",
    "\n",
    "In this case, the orange juice data is available in a public blob container, defined by the information below. In your case, you'll need to specify the account credentials as well. For more information check [the documentation](https://docs.microsoft.com/python/api/azureml-core/azureml.core.datastore.datastore#register-azure-blob-container-workspace--datastore-name--container-name--account-name--sas-token-none--account-key-none--protocol-none--endpoint-none--overwrite-false--create-if-not-exists-false--skip-validation-false--blob-cache-timeout-none--grant-workspace-access-false--subscription-id-none--resource-group-none-)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "blob_datastore_name = \"automl_many_models\"\n",
    "container_name = \"automl-sample-notebook-data\"\n",
    "account_name = \"automlsamplenotebookdata\""
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "automl"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(\n",
    "    workspace=ws, \n",
    "    datastore_name=blob_datastore_name, \n",
    "    container_name=container_name,\n",
    "    account_name=account_name,\n",
    "    create_if_not_exists=True\n",
    ")\n",
    "\n",
    "if 0 < dataset_maxfiles < 11973:\n",
    "    ds_train_path = 'oj_data_small/'\n",
    "    ds_inference_path = 'oj_inference_small/'\n",
    "else:\n",
    "    ds_train_path = 'oj_data/'\n",
    "    ds_inference_path = 'oj_inference/'"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "automl"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.0 Register dataset in AML Workspace\n",
    "\n",
    "The last step is creating and registering [datasets](https://docs.microsoft.com/azure/machine-learning/concept-data#datasets) in Azure Machine Learning for the train and inference sets.\n",
    "\n",
    "Using a [FileDataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.file_dataset.filedataset) is currently the best way to take advantage of the many models pattern, so we create FileDatasets in the next cell. We then [register](https://docs.microsoft.com/azure/machine-learning/how-to-create-register-datasets#register-datasets) the FileDatasets in your Workspace; this associates the train/inference sets with simple names that can be easily referred to later on when we train models and produce forecasts."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# Create file datasets\n",
    "ds_train = Dataset.File.from_files(path=datastore.path(ds_train_path), validate=False)\n",
    "ds_inference = Dataset.File.from_files(path=datastore.path(ds_inference_path), validate=False)\n",
    "\n",
    "# Register the file datasets\n",
    "dataset_name = 'oj_data_small' if 0 < dataset_maxfiles < 11973 else 'oj_data'\n",
    "train_dataset_name = dataset_name + '_train'\n",
    "inference_dataset_name = dataset_name + '_inference'\n",
    "ds_train.register(ws, train_dataset_name, create_new_version=True)\n",
    "ds_inference.register(ws, inference_dataset_name, create_new_version=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.0 *[Optional]* Interact with the registered dataset\n",
    "\n",
    "After registering the data, it can be easily called using the command below. This is how the datasets will be accessed in future notebooks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "oj_ds = Dataset.get_by_name(ws, name=train_dataset_name)\n",
    "oj_ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is also possible to download the data from the registered dataset:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "download_paths = oj_ds.download()\n",
    "download_paths"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load one of the data files to see the format:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_data = pd.read_csv(download_paths[0])\n",
    "sample_data.head(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have created your datasets, you are ready to move to one of the training notebooks to train and score the models:\n",
    "\n",
    "- Automated ML: please open [02_AutoML_Training_Pipeline.ipynb](Automated_ML/02_AutoML_Training_Pipeline/02_AutoML_Training_Pipeline.ipynb).\n",
    "- Custom Script: please open [02_CustomScript_Training_Pipeline.ipynb](Custom_Script/02_CustomScript_Training_Pipeline.ipynb)."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('mm': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "9ffaa93a1da240c6e4760ff6f5da4e26eb0b2149dc0c9afc5ced3714a737930b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}