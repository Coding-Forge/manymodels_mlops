{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/manymodels/02_Training/02_Training_Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline - Automated ML\n",
    "_**Training many models using Automated Machine Learning**_\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates how to train and register 11,973 models using Automated Machine Learning. We will utilize the AutoMLPipelineBuilder to parallelize the process of training 11,973 models. For this notebook we are using an orange juice sales dataset to predict the orange juice quantity for each brand and each store. For more information about the data refer to the Data Preparation Notebook.\n",
    "\n",
    "<span style=\"color:red\"><b>NOTE: There are limits on how many runs we can do in parallel per workspace, and we currently recommend to set the parallelism to maximum of 20 runs per experiment per workspace. If users want to have more parallelism and increase this limit they might encounter Too Many Requests errors (HTTP 429). </b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b> Please ensure you have the latest version of the SDK to ensure AutoML dependencies are consistent.</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade azureml-train-automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the azureml-contrib-automl-pipeline-steps package that is needed for many models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azureml-contrib-automl-pipeline-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have already:\n",
    "\n",
    "1. Created your AML Workspace using the [00_Setup_AML_Workspace notebook](../../00_Setup_AML_Workspace.ipynb)\n",
    "2. Run [01_Data_Preparation.ipynb](../../01_Data_Preparation.ipynb) to create the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Set up workspace, datastore, experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.env_variables import Env\n",
    "from utils.aml_workspace import Connect\n",
    "\n",
    "e=Env()\n",
    "\n",
    "# set up workspace\n",
    "ws = Connect().authenticate()\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()\n",
    "\n",
    "# set up datastores\n",
    "dstore = Datastore.get(ws, e.blob_datastore_name)\n",
    "\n",
    "use_tabular = False\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Default datastore name'] = dstore.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: manymodels-training-pipeline\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment = Experiment(ws, e.experiment_name)\n",
    "print('Experiment name: ' + experiment.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Call the registered filedataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 11,973 datasets and AutoMLPipelineBuilder to build 11,973 time-series to predict the quantity of each store brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset represents a brand's 2 years orange juice sales data that contains 7 columns and 122 rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to register the datasets in the Workspace first. The Data Preparation notebook demonstrates how to register two datasets to the workspace. \n",
    "\n",
    "The registered 'oj_data_small' file dataset contains the first 10 csv files and 'oj_data' contains all 11,973 csv files. You can choose to pass either filedatasets_10_models_input or filedatasets_all_models_inputs in the AutoMLPipelineBuilder.\n",
    "\n",
    "We recommend to **start with filedatasets_10_models** and make sure everything runs successfully, then scale up to filedatasets_all_models.\n",
    "\n",
    "### Option A\n",
    "\n",
    "You can now use Tabular reads of the CSV/Parquet files instead of having to use a File Data Sets.\n",
    "\n",
    "### Option B\n",
    "\n",
    "Using named file data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "if use_tabular:\n",
    "\n",
    "    ds_name_small = \"oj_sales_data_train\"\n",
    "    input_ds_small = Dataset.Tabular.from_delimited_files(\n",
    "        path=dstore.path(ds_name_small + \"/\"), validate=False\n",
    "    )\n",
    "\n",
    "    inference_name_small = \"oj_sales_data_inference\"\n",
    "    inference_ds_small = Dataset.Tabular.from_delimited_files(\n",
    "        path=dstore.path(inference_name_small + \"/\"), validate=False\n",
    "    )\n",
    "else:\n",
    "    filedst_10_models = Dataset.get_by_name(ws, name=\"manymodels_train\")\n",
    "    filedst_10_models_input = filedst_10_models.as_named_input('train_10_models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Build the training pipeline\n",
    "Now that the dataset, WorkSpace, and datastore are set up, we can put together a pipeline for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a compute target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently AutoMLPipelineBuilder only supports AMLCompute. You can change to a different compute cluster if one fails.\n",
    "\n",
    "This is the compute target we will pass into our AutoMLPipelineBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "Checking cluster status...\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = e.compute_name\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=e.vm_size,\n",
    "                                                           min_nodes=e.min_nodes,\n",
    "                                                           max_nodes=e.max_nodes)\n",
    "    # Create the cluster.\n",
    "    compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "compute.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "    \n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a configuration for the data prep cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "env_name = \"dataprep\"\n",
    "env = Environment(env_name)\n",
    "env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "env.docker.enabled = True # Use a docker container\n",
    "\n",
    "# Create a set of package dependencies\n",
    "packages = CondaDependencies.create(conda_packages=['pandas','pip'],\n",
    "                                    pip_packages=['azureml-defaults==1.35'])\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "env.python.conda_dependencies = packages\n",
    "\n",
    "# Register the environment (just in case you want to use it again)\n",
    "env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, env_name)\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = compute\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This dictionary defines the [AutoML settings](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py#parameters), for this forecasting task we add the name of the time column and the maximum forecast horizon.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n",
    "|**blocked_models**|Models in blocked_models won't be used by AutoML. All supported models can be found at [here](https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.constants.supportedmodels.forecasting?view=azure-ml-py).|\n",
    "|**iterations**|Number of models to train. This is optional but provides customer with greater control.|\n",
    "|**iteration_timeout_minutes**|Maximum amount of time in minutes that the model can train. This is optional and depends on the dataset. We ask customer to explore a bit to get approximate times for training the dataset. For OJ dataset we set it 20 minutes|\n",
    "|**experiment_timeout_hours**|Maximum amount of time in hours that the experiment can take before it terminates.|\n",
    "|**label_column_name**|The name of the label column.|\n",
    "|**n_cross_validations**|Number of cross validation splits. Rolling Origin Validation is used to split time-series in a temporally consistent way.|\n",
    "|**enable_early_stopping**|Flag to enable early termination if the score is not improving in the short term.|\n",
    "|**time_column_name**|The name of your time column.|\n",
    "|**max_horizon**|The number of periods out you would like to predict past your training data. Periods are inferred from your data.|\n",
    "|**grain_column_names**|The column names used to uniquely identify timeseries in data that has multiple rows with the same timestamp.|\n",
    "|**partition_column_names**|The names of columns used to group your models. For timeseries, the groups must not split up individual time-series. That is, each group must contain one or more whole time-series.|\n",
    "|**track_child_runs**|Flag to disable tracking of child runs. Only best run (metrics and model) is tracked if the flag is set to False.|\n",
    "|**pipeline_fetch_max_batch_size**|Determines how many pipelines (training algorithms) to fetch at a time for training, this helps reduce throttling when training at large scale.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData, PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "script_folder = \"./scripts\"\n",
    "split_date = PipelineParameter(name=\"DateTime\", default_value=\"2021-01-01 00:00:00\")\n",
    "folder_name = PipelineParameter(name=\"folder_name\", default_value=\"sales_data\")\n",
    "# Create a PipelineData (temporary Data Reference) for the model folder\n",
    "training_data = PipelineData(\"training_output\", datastore=dstore).as_dataset()\n",
    "\n",
    "data_prep = PythonScriptStep(name = \"Step One\",\n",
    "                           script_name=\"training_set.py\",\n",
    "                           source_directory=script_folder,\n",
    "                           runconfig=pipeline_run_config,\n",
    "                           arguments=[\n",
    "                               \"--split_date\", split_date,\n",
    "                               \"--training_output\", training_data],\n",
    "                           outputs=[training_data],\n",
    "                           compute_target = compute,\n",
    "                           allow_reuse = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from azureml.train.automl.runtime._many_models.many_models_parameters import (ManyModelsTrainParameters,)\n",
    "\n",
    "partition_column_names = [e.primary_partition, e.secondary_partition]\n",
    "\n",
    "automl_settings = {\n",
    "    \"task\": \"forecasting\",\n",
    "    \"primary_metric\": \"normalized_root_mean_squared_error\",\n",
    "    \"iteration_timeout_minutes\": 10,  # This needs to be changed based on the dataset. We ask customer to explore how long training is taking before settings this value\n",
    "    \"iterations\": 5,\n",
    "    \"experiment_timeout_hours\": 0.25,\n",
    "    \"label_column_name\": e.label_column_name,\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"time_column_name\": \"DateTime\",  # e.timestamp_column,\n",
    "    #\"drop_column_names\": \"Revenue\",\n",
    "    \"max_horizon\": 6,\n",
    "    \"grain_column_names\": partition_column_names,\n",
    "    \"track_child_runs\": False,\n",
    "}\n",
    "\n",
    "\n",
    "mm_paramters = ManyModelsTrainParameters(\n",
    "    automl_settings=automl_settings, partition_column_names=partition_column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build many model training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoMLPipelineBuilder is used to build the many models train step. You will need to determine the number of workers and nodes appropriate for your use case. The process_count_per_node is based off the number of cores of the compute VM. The node_count will determine the number of master nodes to use, increasing the node count will speed up the training process.\n",
    "\n",
    "* <b>experiment</b>: Current experiment.\n",
    "\n",
    "* <b>automl_settings</b>: AutoML settings dictionary.\n",
    "\n",
    "* <b>train_data</b>: Train dataset.\n",
    "\n",
    "* <b>compute_target</b>: Compute target for training.\n",
    "\n",
    "* <b>partition_column_names</b>: Partition column names.\n",
    "\n",
    "* <b>node_count</b>: The number of compute nodes to be used for running the user script. We recommend to start with 3 and increase the node_count if the training time is taking too long.\n",
    "\n",
    "* <b>process_count_per_node</b>: The number of processes per node.\n",
    "\n",
    "* <b>run_invocation_timeout</b>: The run() method invocation timeout in seconds. The timeout should be set to maximum training time of one AutoML run(with some buffer), by default it's 60 seconds.\n",
    "\n",
    "* <b>output_datastore</b>: Output datastore to output the training results.\n",
    "\n",
    "* <b>train_env(Optional)</b>: Optionally can provide train environment definition to use for training.\n",
    "\n",
    "<span style=\"color:red\"><b>NOTE: There are limits on how many runs we can do in parallel per workspace, and we currently recommend to set the parallelism to maximum of 320 runs per experiment per workspace. If users want to have more parallelism and increase this limit they might encounter Too Many Requests errors (HTTP 429). </b></span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azureml.contrib.automl.pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.get(ws, \"AzureML-AutoML\", label=\"Latest\")\n",
    "env.environment_variables = {'AZUREML_OUTPUT_UPLOAD_TIMEOUT_SEC':'7200'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataException",
     "evalue": "DataException:\n\tMessage: Request for [<class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset'>] is not supported.\n\tInnerException: None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Request for [<class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset'>] is not supported.\",\n        \"target\": \"input_dataset\",\n        \"inner_error\": {\n            \"code\": \"NotSupported\"\n        },\n        \"reference_code\": \"70933964-4f38-40e4-4621-0dadfa83db85\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataException\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12629/3177537777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                 \u001b[0mprocess_count_per_node\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                 \u001b[0mrun_invocation_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m920\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                                 train_pipeline_parameters=mm_paramters, output_datastore=dstore)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/contrib/automl/pipeline/steps/automl_pipeline_builder.py\u001b[0m in \u001b[0;36mget_many_models_train_steps\u001b[0;34m(experiment, train_data, compute_target, node_count, automl_settings, partition_column_names, process_count_per_node, run_invocation_timeout, train_pipeline_parameters, output_datastore, train_env, arguments)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             mm_input = ManyModelsInputDataset.from_input_data(\n\u001b[0;32m--> 148\u001b[0;31m                 train_data, partition_column_names, input_dataset_name=\"many_models_train_data\")\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmm_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_partition_step_needed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/train/automl/runtime/_many_models/many_models_input_dataset.py\u001b[0m in \u001b[0;36mfrom_input_data\u001b[0;34m(input_data, partition_column_names, input_dataset_name, use_train_level)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0minput_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mconsumption_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetConsumptionConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0minput_dataset_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhts_client_utilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_dataset_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_column_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         return ManyModelsInputDataset(\n\u001b[1;32m    138\u001b[0m             \u001b[0minput_dataset_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/train/automl/_hts/hts_client_utilities.py\u001b[0m in \u001b[0;36mget_input_dataset_type\u001b[0;34m(dataset, hierarchy)\u001b[0m\n\u001b[1;32m    447\u001b[0m         AzureMLError.create(\n\u001b[1;32m    448\u001b[0m             \u001b[0mNotSupported\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mreference_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mReferenceCodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTS_DATASET_NOT_SUPPORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    451\u001b[0m     )\n",
      "\u001b[0;31mDataException\u001b[0m: DataException:\n\tMessage: Request for [<class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset'>] is not supported.\n\tInnerException: None\n\tErrorResponse \n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Request for [<class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset'>] is not supported.\",\n        \"target\": \"input_dataset\",\n        \"inner_error\": {\n            \"code\": \"NotSupported\"\n        },\n        \"reference_code\": \"70933964-4f38-40e4-4621-0dadfa83db85\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.contrib.automl.pipeline.steps import AutoMLPipelineBuilder\n",
    "\n",
    "\n",
    "if use_tabular:\n",
    "\n",
    "    train_steps = AutoMLPipelineBuilder.get_many_models_train_steps(\n",
    "                                                                experiment=experiment,\n",
    "                                                                train_data=input_ds_small,\n",
    "                                                                compute_target=compute,\n",
    "                                                                node_count=2,\n",
    "                                                                process_count_per_node=8,\n",
    "                                                                run_invocation_timeout=920,\n",
    "                                                                train_pipeline_parameters=mm_paramters, output_datastore=dstore)\n",
    "\n",
    "else:\n",
    "\n",
    "    train_steps = AutoMLPipelineBuilder.get_many_models_train_steps(\n",
    "                                                                experiment=experiment,\n",
    "                                                                train_data=training_data, # filedst_10_models_input,\n",
    "                                                                compute_target=compute,\n",
    "                                                                node_count=2,\n",
    "                                                                process_count_per_node=8,\n",
    "                                                                run_invocation_timeout=920,\n",
    "                                                                train_pipeline_parameters=mm_paramters, output_datastore=dstore)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [data_prep, train_steps]\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    " \n",
    "# Create an experiment and run the pipeline\n",
    "#experiment = Experiment(workspace = ws, name = 'pipeline')\n",
    "pipeline_run = Experiment(workspace=ws, name='pipeline').submit(pipeline,parameters={'suffix':'dev', 'Last_Actual_Date':'2021-07-01 00:00:00'})\n",
    "#pipeline_run = experiment.submit(pipeline, regenerate_outputs=True).parameters({'suffix':'dev', 'Last_Actual_Date':'2021-07-01 00:00:00'})\n",
    "print(\"Pipeline submitted for execution.\")\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Run the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we submit our pipeline to run. The whole training pipeline takes about 1h 11m using a STANDARD_D16S_V3 VM with our current AutoMLPipelineBuilder setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.get(\"BUILDID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline_steps = [data_prep, train_steps]\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "print(\"Pipeline is built.\")\n",
    "\n",
    "pipelineName = e.experiment_name\n",
    "pipeline_run = Experiment(ws, pipelineName).submit(pipeline,  tags={\"BuildId\": os.environ.get(\"BUILDID\"), \"ComputeName\": e.vm_size})\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the folowing command if you'd like to monitor the training process in jupyter notebook. It will stream logs live while training. \n",
    "\n",
    "**Note**: This command may not work for Notebook VM, however it should work on your local laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.wait_for_completion(show_output=True)\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Succesfully trained, registered Automated ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Review outputs of the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training pipeline will train and register models to the Workspace. You can review trained models in the Azure Machine Learning Studio under 'Models'.\n",
    "If there are any issues with training, you can go to 'many-models-training' run under the pipeline run and explore logs under 'Logs'.\n",
    "You can look at the stdout and stderr output under logs/user/worker/<ip> for more details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Get list of AutoML runs along with registered model names and tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will iterate through all the automl runs for the experiment and list the details.\n",
    "\n",
    "**Framework** - AutoML, **Dataset** - input data set, **Run** - AutoML run id, **Status** - AutoML run status,  **Model** - Registered model name, **Tags** - Tags for model, **StartTime** - Start time, **EndTime** - End time, **ErrorType** - ErrorType, **ErrorCode** - ErrorCode, **ErrorMessage** - Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper import get_training_output\n",
    "import os\n",
    "\n",
    "training_results_name = \"training_results\"\n",
    "training_output_name = \"many_models_training_output\"\n",
    "\n",
    "#training_file = get_training_output(run, training_results_name, training_output_name)\n",
    "training_file = get_training_output(pipeline_run, training_results_name, training_output_name)\n",
    "\n",
    "all_columns = [\"Framework\", \"Dataset\", \"Run\", \"Status\", \"Model\", \"Tags\", \"StartTime\", \"EndTime\" , \"ErrorType\", \"ErrorCode\", \"ErrorMessage\" ]\n",
    "df = pd.read_csv(training_file, delimiter=\" \", header=None, names=all_columns)\n",
    "training_csv_file = \"training.csv\"\n",
    "df.to_csv(training_csv_file)\n",
    "print(\"Training output has\", df.shape[0], \"rows. Please open\", os.path.abspath(training_csv_file), \"to browse through all the output.\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Publish and schedule the pipeline (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Publish the pipeline\n",
    "\n",
    "Once you have a pipeline you're happy with, you can publish a pipeline so you can call it programmatically later on. See this [tutorial](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-your-first-pipeline#publish-a-pipeline) for additional information on publishing and calling pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "pipelineEndpointName = 'automl_train_many_models'\n",
    "\n",
    "published_pipeline = pipeline.publish(name = 'automl_train_many_models',\n",
    "                                  description = 'train many models',\n",
    "                                  version = '1',\n",
    "                                  continue_on_step_failure = False)\n",
    "\n",
    "\n",
    "if pipelineEndpointName in str(PipelineEndpoint.list(ws)):\n",
    "    # Add a new Version to an existing Endpoint\n",
    "    pipeline_endpoint = PipelineEndpoint.get(workspace = ws, name = pipelineEndpointName)\n",
    "    pipeline_endpoint.add_default(published_pipeline)\n",
    "else:\n",
    "    # Create a new Endpoint\n",
    "    pipeline_endpoint = PipelineEndpoint.publish(workspace = ws,\n",
    "                                                name = pipelineEndpointName,\n",
    "                                                pipeline = published_pipeline,\n",
    "                                                description = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_run.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Schedule the pipeline\n",
    "You can also [schedule the pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-schedule-pipelines) to run on a time-based or change-based schedule. This could be used to automatically retrain models every month or based on another trigger such as data drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "    \n",
    "# training_pipeline_id = published_pipeline.id\n",
    "\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Month\", interval=1, start_time=\"2020-01-01T09:00:00\")\n",
    "# recurring_schedule = Schedule.create(ws, name=\"automl_training_recurring_schedule\", \n",
    "#                             description=\"Schedule Training Pipeline to run on the first day of every month\",\n",
    "#                             pipeline_id=training_pipeline_id, \n",
    "#                             experiment_name=experiment.name, \n",
    "#                             recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Bookkeeping of workspace (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Cancel any runs that are running\n",
    "\n",
    "To cancel any runs that are still running in a given experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.helper import cancel_runs_in_experiment\n",
    "# failed_experiment =  'Please modify this and enter the experiment name'\n",
    "# # Please note that the following script cancels all the currently running runs in the experiment\n",
    "# cancel_runs_in_experiment(ws, failed_experiment)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "deeptim"
   }
  ],
  "interpreter": {
   "hash": "3fec610cbb67958716dc318121910cfa04ded6f5645ced1eebbb9789e5469472"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('azureml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
