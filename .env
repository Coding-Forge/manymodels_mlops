# Azure Subscription Variables
# create you environment variable based on your OS. Use the ${} to 
# read in environment variables already defined on your machine

SUBSCRIPTION_ID = ${SUBSCRIPTION_ID} 
LOCATION = 'eastus'
TENANT_ID = ''
BASE_NAME = 'PythonMLOps'
SP_APP_ID = ''
SP_APP_SECRET = ''
RESOURCE_GROUP = 'coding-forge-rg'

# Mock build/release ID for local testing
BUILD_BUILDID = '001'

# Azure ML Workspace Variables
WORKSPACE_NAME = 'coding-forge-ml-ws'
EXPERIMENT_NAME = 'manymodels-training-pipeline'

# AML Associated Storage and structure names
BLOB_DATASTORE_NAME     = "poc_blob_store"
CONTAINER_NAME          = "poccontainer"
ACCOUNT_NAME            = "codingforgesa"

# AML Compute Cluster Config
AML_ENV_NAME=''
AML_ENV_TRAIN_CONDA_DEP_FILE="conda_dependencies.yml"
AML_COMPUTE_CLUSTER_NAME = 'cpucluster'
AML_COMPUTE_CLUSTER_CPU_SKU = 'STANDARD_D13_V2'
AML_CLUSTER_MAX_NODES = '20'
AML_CLUSTER_MIN_NODES = '0'
AML_CLUSTER_PRIORITY = 'lowpriority'
# Training Config
MODEL_NAME = 'diabetes_regression_model.pkl'
MODEL_VERSION = '1'
TRAIN_SCRIPT_PATH = 'training/train_aml.py'


# AML Pipeline Config
TRAINING_PIPELINE_NAME = 'Training Pipeline'
MODEL_PATH = ''
EVALUATE_SCRIPT_PATH = 'evaluate/evaluate_model.py'
REGISTER_SCRIPT_PATH = 'register/register_model.py'
SOURCES_DIR_TRAIN = 'diabetes_regression'
DATASET_NAME = 'oj_data_small_train'
DATASET_VERSION = 'latest'
# Optional. Set it if you have configured non default datastore to point to your data
DATASTORE_NAME = 'workspaceblobstore'
SCORE_SCRIPT = 'scoring/score.py'

# Optional. Used by a training pipeline with R on Databricks
DB_CLUSTER_ID = ''

# Optional. Container Image name for image creation
IMAGE_NAME = 'mltrained'

# Run Evaluation Step in AML pipeline
RUN_EVALUATION = 'true'

# Set to true cancels the Azure ML pipeline run when evaluation criteria are not met.
ALLOW_RUN_CANCEL = 'true'

# Flag to allow rebuilding the AML Environment after it was built for the first time. This enables dependency updates from conda_dependencies.yaml.
AML_REBUILD_ENVIRONMENT = 'false'

USE_GPU_FOR_SCORING = "false"
AML_ENV_SCORE_CONDA_DEP_FILE="conda_dependencies_scoring.yml"
AML_ENV_SCORECOPY_CONDA_DEP_FILE="conda_dependencies_scorecopy.yml"
# AML Compute Cluster Config for parallel batch scoring
AML_ENV_NAME_SCORING=''
AML_ENV_NAME_SCORE_COPY=''
AML_COMPUTE_CLUSTER_NAME_SCORING = 'score-cluster'
AML_COMPUTE_CLUSTER_CPU_SKU_SCORING = 'STANDARD_DS2_V2'
AML_CLUSTER_MAX_NODES_SCORING = '4'
AML_CLUSTER_MIN_NODES_SCORING = '0'
AML_CLUSTER_PRIORITY_SCORING = 'lowpriority'
AML_REBUILD_ENVIRONMENT_SCORING = 'true'
BATCHSCORE_SCRIPT_PATH = 'scoring/parallel_batchscore.py'
BATCHSCORE_COPY_SCRIPT_PATH = 'scoring/parallel_batchscore_copyoutput.py'


SCORING_DATASTORE_INPUT_CONTAINER = 'input'
SCORING_DATASTORE_INPUT_FILENAME = 'diabetes_scoring_input.csv'
SCORING_DATASTORE_OUTPUT_CONTAINER = 'output'
SCORING_DATASTORE_OUTPUT_FILENAME = 'diabetes_scoring_output.csv'
SCORING_DATASET_NAME = 'diabetes_scoring_ds'
SCORING_PIPELINE_NAME = 'diabetes-scoring-pipeline'


# Other variables that are more specific to each project
# 'WeekStarting'
TIMESTAMP_COLUMN = 'Date_Time'

# 'Store'
PRIMARY_PARTITION = 'LocationNumber'

# 'Brand'
SECONDARY_PARTITION = 'item_id'

# '1992-05-28'
SPLIT_DATE = '2021-11-03'

LABEL_COLUMN_NAME = "Label_Column"